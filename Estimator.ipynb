{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "How to open and understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_derivative(a,y):\n",
    "    return (a-y)/(a*(1-a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basic information\n",
    "1. Hyperspectral data:\n",
    "    1. `hsi_path` contains path to hyperspectral masked numpy arrays containing hyperspectral data that underwent masking (i.e., the field area is masked, whereas all irrelevant areas are not masked)\n",
    "    2. The name of the file (e.g., _'1989.npz'_) refers to the index of the corresponding training sample in the ground-truth table.\n",
    "2. Ground-truth data:\n",
    "    1. `gt_path` contains path to ground truth .csv file.\n",
    "    2. Additionally, `wavelength_path` contains the mapping between a band number and the corresponding wavelength.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hsi_path = 'Desktop/train_data/train_data/1331.npz'\n",
    "gt_path = 'Desktop/train_data/train_gt.csv'\n",
    "wavelength_path = 'Desktop/train_data/wavelengths.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gt_df = pd.read_csv(gt_path)\n",
    "gt_df\n",
    "wavelength_df = pd.read_csv(wavelength_path)\n",
    "wavelength_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ground-truth description\n",
    "`gt_df` contains:\n",
    "\n",
    "1. `sample_index` - a reference to the numpay array containing the corresponding hyperspectral data.\n",
    "2. P (for simplicity, we use P while referring to P_2O_5), K, Mg, pH - soil properties levels based on laboratory measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gt_df[gt_df['sample_index']==1331]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Displaying one hyperspectral band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "band_id = 100\n",
    "wavelength = wavelength_df.loc[band_id-1]\n",
    "with np.load(hsi_path) as npz:\n",
    "    arr = np.ma.MaskedArray(**npz)\n",
    "\n",
    "axs[0].imshow(arr[band_id].data)\n",
    "axs[1].imshow(arr[band_id])\n",
    "plt.suptitle(f'Representation of band {int(wavelength[\"band_no\"])} ({wavelength[\"wavelength\"]} nm)', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the aggregated spectral curve for a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "masked_scene_mean_spectral_reflectance = [arr[i].mean() for i in range(arr.shape[0])]\n",
    "full_scene_mean_spectral_reflectance = [arr[i].data.mean() for i in range(arr.shape[0])]\n",
    "\n",
    "plt.plot(wavelength_df['wavelength'], full_scene_mean_spectral_reflectance, label='Full image')\n",
    "plt.plot(wavelength_df['wavelength'], masked_scene_mean_spectral_reflectance, label='Masked image')\n",
    "\n",
    "plt.xlabel('[nm]')\n",
    "plt.legend()\n",
    "plt.title(f'Average reflectance ({hsi_path.split(\"/\")[-1]})')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating baseline solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingNet:\n",
    "\n",
    "    def __init__(self,sizes:list or np.array):\n",
    "        self.num_layers=len(sizes)\n",
    "        self.sizes=sizes\n",
    "        self.weights=[np.random.randn(y,x)/np.sqrt(x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "        self.biases=[np.random.randn(y,1) for y in sizes[1:]]\n",
    "    \n",
    "    def feedforward(self,a:np.array or list):\n",
    "        for w,b in zip(self.weights,self.biases):\n",
    "            a=sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "    \n",
    "    def LayerByLayerFF(self,X):\n",
    "        Zs=[]\n",
    "        As=[X]\n",
    "        a=X\n",
    "        for w,b in zip(self.weights,self.biases):\n",
    "            z=np.dot(w,a) + b\n",
    "            Zs.append(z)\n",
    "            a=sigmoid(z)\n",
    "            As.append(a)\n",
    "        return Zs,As\n",
    "    \n",
    "    def update_wb(self,X,Y,eta):  #X and Y are matrices\n",
    "        nabla_w,nabla_b=self.backprop(X,Y)\n",
    "        self.weights=[w-(eta/X.shape[1])*dw for w,dw in zip(self.weights,nabla_w)]\n",
    "        self.biases=[b-(eta/X.shape[1])*db for b,db in zip(self.biases,nabla_b)]\n",
    "    \n",
    "    def backprop(self,X,Y):\n",
    "        Z,A=self.LayerByLayerFF(X)  # A collection of matrices.Each matrix corresponds to a layer of z(s) and a(s) for all inputs X.\n",
    "        del_Ls=cost_derivative(A[-1],Y)*sigmoid_prime(Z[-1])\n",
    "        deltas=[del_Ls]\n",
    "        for i,(w,z) in enumerate(zip(self.weights[-1:0:-1],Z[-2::-1])):\n",
    "            del_prev=np.dot(w.transpose(),deltas[i])*sigmoid_prime(z)\n",
    "            deltas.append(del_prev)\n",
    "        #Note:deltas contains the \"errors\" of each layer for all training examples (simultaneously) in reverse.\n",
    "        #i.e deltas[0] has the \"errors\" of the last layer\n",
    "        do_w=[]\n",
    "        do_b=[]\n",
    "        for delta_out,a_in in zip(deltas[-1::-1],A[:-1]):\n",
    "            d_b=np.sum(delta_out,axis=1).reshape((len(delta_out),1))\n",
    "            d_w=np.dot(delta_out,a_in.transpose()) \n",
    "            do_b.append(d_b)\n",
    "            do_w.append(d_w)        \n",
    "        return do_w,do_b                  \n",
    "    \n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        #X_test=[150x252] x=150x1 ====> y=4x1=>inv_sigmoid(y)\n",
    "        X_test=X_test.transpose()\n",
    "        predictions=self.feedforward(X_test)             \n",
    "        return predictions.transpose()\n",
    "        \n",
    "\n",
    "class BaselineRegressor:\n",
    "    \"\"\"\n",
    "    Baseline regressor, which calculates the mean value of the target from the training\n",
    "    data and returns it for each testing sample.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean = 0\n",
    "\n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        self.mean = np.mean(y_train, axis=0)\n",
    "        self.classes_count = y_train.shape[1]  #len=4\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test: np.ndarray):\n",
    "        return np.full((len(X_test), self.classes_count), self.mean)\n",
    "\n",
    "\n",
    "class SpectralCurveFiltering():\n",
    "    \"\"\"\n",
    "    Create a histogram (a spectral curve) of a 3D cube, using the merge_function\n",
    "    to aggregate all pixels within one band. The return array will have\n",
    "    the shape of [CHANNELS_COUNT]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, merge_function = np.mean):\n",
    "        self.merge_function = merge_function\n",
    "\n",
    "    def __call__(self, sample: np.ndarray):\n",
    "        return self.merge_function(sample[58:74], axis=(1, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def load_data(directory: str):\n",
    "    \"\"\"Load each cube, reduce its dimensionality and append to array.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory to either train or test set\n",
    "    Returns:\n",
    "        [type]: A list with spectral curve for each sample.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    filtering = SpectralCurveFiltering()\n",
    "    all_files = np.array(\n",
    "        sorted(\n",
    "            glob(os.path.join(directory, \"*.npz\")),\n",
    "            key=lambda x: int(os.path.basename(x).replace(\".npz\", \"\")),\n",
    "        )\n",
    "    )\n",
    "    for file_name in all_files:\n",
    "        with np.load(file_name) as npz:\n",
    "            arr = np.ma.MaskedArray(**npz)\n",
    "        arr = filtering(arr)\n",
    "        data.append(arr/100)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def load_gt(file_path: str):\n",
    "    \"\"\"Load labels for train set from the ground truth file.\n",
    "    Args:\n",
    "        file_path (str): Path to the ground truth .csv file.\n",
    "    Returns:\n",
    "        [type]: 2D numpy array with soil properties levels\n",
    "    \"\"\"\n",
    "    gt_file = pd.read_csv(file_path)\n",
    "    labels = gt_file[[\"P\", \"K\", \"Mg\", \"pH\"]].values\n",
    "    return labels\n",
    "\n",
    "\n",
    "X_train = load_data(\"train_data\")\n",
    "y_train = load_gt(\"train_gt.csv\")\n",
    "X_test = load_data(\"test_data\")\n",
    "print(X_train.shape)\n",
    "#print(y_train)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions and generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_reg = BaselineRegressor()\n",
    "baseline_reg = baseline_reg.fit(X_train, y_train)\n",
    "predictions = baseline_reg.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame(data = predictions, columns=[\"P\", \"K\", \"Mg\", \"pH\"])\n",
    "submission.to_csv(\"submission.csv\", index_label=\"sample_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the metric\n",
    "\n",
    "For the purpose of presenting the final metric calculation, we will extract a small _test_set_ from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train[1500:]\n",
    "#print(X_test.shape)\n",
    "y_test = y_train[1500:]\n",
    "X_train_new = X_train[:]\n",
    "y_train_new = y_train[:]\n",
    "\n",
    "# Fit the baseline regressor once again on new training set\n",
    "\n",
    "baseline_reg = baseline_reg.fit(X_train_new, y_train_new)\n",
    "baseline_predictions = baseline_reg.predict(X_test)\n",
    "\n",
    "# Generate baseline values to be used in score computation\n",
    "baselines = np.mean((y_test - baseline_predictions) ** 2, axis=0)\n",
    "\n",
    "diff_P=max(y_train_new[:,0]) - min(y_train_new[:,0])\n",
    "min_P=min(y_train_new[:,0])\n",
    "\n",
    "diff_K=max(y_train_new[:,1]) - min(y_train_new[:,1])\n",
    "min_K=min(y_train_new[:,1])\n",
    "\n",
    "diff_M=max(y_train_new[:,2]) - min(y_train_new[:,2])\n",
    "min_M=min(y_train_new[:,2])\n",
    "\n",
    "diff_p=max(y_train_new[:,3]) - min(y_train_new[:,3])\n",
    "min_p=min(y_train_new[:,3])\n",
    "\n",
    "y_train_new_2=np.zeros(y_train_new.transpose().shape)\n",
    "i=0\n",
    "for y,Diff,Min in zip(y_train_new.transpose(),[diff_P,diff_K,diff_M,diff_p],[min_P,min_K,min_M,min_p]):\n",
    "    y_train_new_2[i]=(y-Min)/Diff\n",
    "    i+=1\n",
    "# Generate random predictions, different from baseline predictions\n",
    "mini_batch_size=433\n",
    "test=TestingNet([16,30,4])\n",
    "#SGD training:\n",
    "X_test.transpose())\n",
    "i=0\n",
    "predictions2=np.zeros(predictions.shape)\n",
    "for p,Diff,Min in zip(predictions,[diff_P,diff_K,diff_M,diff_p],[min_P,min_K,min_M,min_p]):\n",
    "    predictions2[i]=p*(Diff) + Min\n",
    "    i+=1\n",
    "predictions=predictions2.transpose()\n",
    "# Calculate MSE for each class\n",
    "mse = np.mean((y_test - predictions) ** 2, axis=0)\n",
    "# Calculate the score for each class individually\n",
    "scores = mse / baselines\n",
    "\n",
    "# Calculate the final score\n",
    "final_score = np.mean(scores)\n",
    "\n",
    "for score, class_name in zip(scores, [\"P\", \"K\", \"Mg\", \"pH\"]):\n",
    "    print(f\"Class {class_name} score: {score}\")\n",
    "\n",
    "print(f\"Final score: {final_scfor epoch in range(30):\n",
    "    mini_batches=np.array([X_train_new.transpose()[:,k:k+mini_batch_size] for k in range(0,len(X_train_new),mini_batch_size)])\n",
    "    mini_batches_out=np.array([y_train_new_2[:,k:k+mini_batch_size] for k in range(0,len(X_train_new),mini_batch_size)])\n",
    "    for mini_batch,out in zip(mini_batches,mini_batches_out):\n",
    "        test.update_wb(mini_batch,out,0.5)\n",
    "\n",
    "    \n",
    "predictions=test.feedforward(ore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = load_data(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=test.feedforward(X_test.transpose())\n",
    "i=0\n",
    "predictions2=np.zeros(predictions.shape)\n",
    "for p,Diff,Min in zip(predictions,[diff_P,diff_K,diff_M,diff_p],[min_P,min_K,min_M,min_p]):\n",
    "    predictions2[i]=p*(Diff) + Min\n",
    "    i+=1\n",
    "predictions=predictions2.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data = predictions, columns=[\"P\", \"K\", \"Mg\", \"pH\"])\n",
    "submission.to_csv(\"submission.csv\", index_label=\"sample_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
